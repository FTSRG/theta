---
title: "IDA Homework - Analysis of Formal Verification Algorithms"
author: "Akos Hajdu"
date: '2016'
output:
  html_document:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_depth: 2
  pdf_document:
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_depth: 2
classoption: a4paper
---
# Introduction

## Background

In my research, I am working on the formal verification of software and hardware systems. I create a formal model (a formal representation) of the system and the desired property using first order logic formulas. Then, the set of possible states of the system (state space) can be explored to check whether it is safe, i.e., the desired property holds for each reachable state (e.g., a variable `x` should never be negative during the execution of a software). However, a major drawback of formal verification techniques is the so-called state space explosion problem, which means that the set of possible states of a system can be unmanageably or even infinitely large. To overcome this problem, I use abstraction-based techniques, which reduce complexity by hiding information that is not relevant for verification. However, finding the proper precision of abstraction is a difficult task. Counterexample-Guided Abstraction Refinement (CEGAR) is an automatic verification algorithm that starts with a coarse abstraction and refines it iteratively until the proper precision is obtained. CEGAR is a general concept, having numerous variants in the literature, with different variants performing better for different tasks. The main focus of my research is the development of a generic CEGAR framework that can incorporate different variants of the CEGAR algorithm. This way, the most appropriate variant can be choosed for a particular verification task. In this homework I run several variants of the algorithm on different models and analyze the results.

## Variables of the problem

### Input variables

* Parameters of the model (the formal representation of the system under verification)
    + **Type** of the model (hardware or plc)
    + Name of the **model**
    + Number of **variables** in the model
    + **Size** of the formulas describing the model
* Parameters of the algorithm (the algorithm configuration)
    + Abstract **domain** (predicate or explicit)
    + **Refinement** strategy (Craig interpolation, sequence interpolation, unsat core)
    + **Initial precision** of the abstraction (empty or property-based)
    + **Search** strategy (BFS or DFS)

Constraints: unsat core refinement cannot be used with predicate domain. Besides that, all combinations of the algorithm parameters are valid configurations.

### Output variables

* Is the model **safe**, i.e., does the property hold
* Execution **time** of the verification
* Number of refinement **iterations**
* **Size** of the ARG (Abstract Reachability Graph, i.e., the abstract state space)
* **Depth** of the ARG
* **Length** of the counterexample (if the model is not safe)

Constraints: it is possible that the algorithm did not terminate within a specified time (see later). In this case all output variables are `NA` values. Furthermore, the length of the counterexample is `NA` if the model is safe.

# Summary of the data

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(partykit)
library(cluster)
library(corrgram)
library(lubridate)
library(gridExtra)
```

Common variables for the script:

```{r}
timeout.ms <- 480000                           # Timeout used during the measurements
csv.path = "log_20161209_153530_hw_plc_vm.csv" # Path of the data

```

## Load and clean data
 
```{r}
d <- read.csv(csv.path, header = T, sep = ",", quote = "", na.strings = "")
# Formatting: trim the name of the model, determine new variable 'Type'
d$Model <- as.factor(gsub("models/", "", d$Model))
d$Model <- as.factor(substr(d$Model, 0, regexpr("\\.[^\\.]*$", d$Model) - 1))
d <- data.frame(Type = as.factor(substr(d$Model, 0, regexpr('/', d$Model) - 1)), d)
# Filter timeouts
d.no.to <- d %>% filter(!is.na(TimeMs))
```

## Summary

```{r, include = FALSE}
models.count <- length(unique(d$Model))
configs.count <- nrow(unique(select(d, Domain, Refinement, InitPrec, Search)))
total.time <- seconds_to_period(sum(ifelse(is.na(d$TimeMs), timeout.ms, d$TimeMs))/1000)
```

There are **`r models.count`** input models and **`r configs.count`** algorithm configurations. Each measurement was repeated **`r nrow(d)/models.count/configs.count`** times, yielding a total number of **`r nrow(d)`** measurement points with **`r nrow(d.no.to)`** successful verifications (non-timeouts) (**`r round(nrow(d.no.to)/nrow(d)*100)`%**). Total time of the measurements: **`r total.time$day` days `r total.time$hour` hours**.

```{r}
str(d, width = 80, strict.width = "cut") # Check column types
summary(d) # Check summary values
```

## Histograms of output variables

The following histograms show an overview of the distribution of the output variables.

```{r}
plots = list()
plots[[1]] <- qplot(d.no.to$Safe,
                    xlab = "Safety")
plots[[2]] <- qplot(d.no.to$TimeMs, geom = "histogram", bins = 10,
                    xlab = "Execution time (ms)")
plots[[3]] <- qplot(d.no.to$Iterations, geom = "histogram", bins = 10,
                    xlab = "Iterations")
plots[[4]] <- qplot(d.no.to$ARGsize, geom = "histogram", bins = 10,
                    xlab = "ARG size")
plots[[5]] <- qplot(d.no.to$ARGdepth, geom = "histogram", bins = 10,
                    xlab = "ARG depth")
plots[[6]] <- qplot(filter(d.no.to, !is.na(d.no.to$CEXlen))$CEXlen,
                    geom = "histogram", bins = 20, xlab = "Counterexample length")
do.call(grid.arrange, plots)
```

# Basic analysis

## Number of successful executions

The following plot shows that each model was verified by at least one configuration. It is interesting that there was a model (plc3), which could only be verified by a single configuration. It can also be observed, that either all repetitions of a measurement succeeded or all timed out. Regarding the configurations, it can be seen that some of them can verify almost every model, while others can only verify about half of them. However, none of the configurations could verify every model.

```{r}
# Combine parameter columns into a single string column
d.confs <- data.frame(d, Config = paste(d$Domain, d$Refinement,
                                        d$InitPrec, d$Search, sep = " "))
# Calculate number of successful runs for each configuration and model
model.conf.succ <- d.confs %>% group_by(Config, Model) %>%
  summarise(Success = sum(ifelse(is.na(Safe), 0, 1)))
# Plot results
ggplot(model.conf.succ, aes(Model, Config, fill = Success)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "red", high = "green") +
  labs(title = "Number of successful (non-timeout) executions",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Safety results

The following plot shows that (1) all configurations agree on the safety of the model (whether it meets the property or not) and (2) either all executions of a configuration yielded safe or all yielded unsafe. The lack of the previous properties would obviously mean that the algorithms are not sound.

```{r}
# Calculate number of safe results for each configuration and model
model.conf.safe <- d.confs %>% group_by(Config, Model) %>%
  summarise(SafeSum = sum(ifelse(Safe == "true", 1, 0)))
# Plot results
ggplot(model.conf.safe, aes(Model, Config, fill = SafeSum)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "red", high = "green", na.value = "white") +
  labs(title = "Number of executions that yielded a safe result",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Average execution times

The following plot shows that there are 2-3 orders of magnitude difference between average execution times. It is not surprising that the same configuration performs differently for different models, but it is interesting that for certain models there is a great difference in performance with the change of a single parameter of the configuration. It is also quite surprising that there is a model, where only one configuration was successful, but with a short execution time. This case could be the subject of detailed analysis in the future.

```{r}
# Calculate average execution time and RSD for each configuration and model
model.conf.time <- d.confs %>% group_by(Config, Model) %>%
  summarise(AvgTime = log10(mean(TimeMs)), RSD = sd(TimeMs)/mean(TimeMs))
# Plot average time
ggplot(model.conf.time, aes(Model, Config, fill = AvgTime)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  labs(title = "Average execution time (log10, milliseconds)",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Relative standard deviation of measurements

The following plot shows that the relative standard deviation of the repeated executions is low (maximum is `r max(model.conf.time$RSD, na.rm = TRUE)`). This indicates a robust measurement environment.

```{r}
# Plot RSD
ggplot(model.conf.time, aes(Model, Config, fill = RSD)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  labs(title = "Relative standard deviation (RSD) of time",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# Correlations

## On the whole data set

The following correlation matrices and correlograms show some correlation between variables, but it is not convincing enough. Therefore, I split the results into two parts based on the type of the model (hardware or PLC).

```{r}
# Select numerical data
d.corr <- select(d, TimeMs, Vars, Size, Iterations, ARGsize, ARGdepth, CEXlen)
# Correlation matrix
round(cor(d.corr, use = "pairwise.complete.obs"), 4)
# Correlograms
corrgram(d.corr, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
corrgram(d.corr, order = TRUE, lower.panel = panel.ellipse, upper.panel = panel.pts)
```

## On hardware models

The following matrices and correlograms show some strong correlations.

* Correlation between the number of variables and the size of the model is due to the formal representation of hardware circuits (e.g., for each gate output, a unique variable is assigned).
* The correlation between the depth of the ARG and the length of the counterexample may indicate that counterexamples are always on the deepest level of the explored state space.
* Correlation between the number of iterations and the counterexample length indicates that for each step towards the target state, a refinement was required.

```{r}
# Filter to hardware
d.corr.hw <- select(filter(d, Type == "hw"), TimeMs, Vars, Size, Iterations, ARGsize,
                    ARGdepth, CEXlen)
# Correlation matrix
round(cor(d.corr.hw, use = "pairwise.complete.obs"), 4)
# Correlogram
corrgram(d.corr.hw, order = TRUE, lower.panel = panel.pts, upper.panel = panel.pie)

```

## On PLC models

Correlations for PLC models are less significant. This may be due to the high diversity of PLC models.

```{r}
# Filter to PLC
d.corr.plc <- select(filter(d, Type == "plc"), TimeMs, Vars, Size, Iterations, ARGsize,
                     ARGdepth, CEXlen)
# Correlation matrix
round(cor(d.corr.plc, use = "pairwise.complete.obs"), 4)
# Correlogram
corrgram(d.corr.plc, order = TRUE, lower.panel = panel.pts, upper.panel = panel.pie)
```

## Linear regressions

Based on the correlations above, some linear regressions were also calculated. The R-squared values show a strong correlation (R-squared > 0.8).

```{r}
# Variables ~ Size (hardware)
ggplot(d.corr.hw, aes(Vars, Size)) +
  geom_point(alpha = 1/5, size = 3) +
  labs(title = "Linear regression between variables and size (hardware)",
       x = "Variables", y = "Size") +
  geom_smooth(method = "lm")

summary(lm(d.corr.hw$Size ~ d.corr.hw$Vars))

# Iterations ~ Time (PLC)
d.corr.plc.no.to <- filter(d.corr.plc, !is.na(TimeMs))
ggplot(d.corr.plc.no.to, aes(Iterations, TimeMs)) +
  geom_point(alpha = 1/5, size = 3) +
  labs(title = "Linear regression between iterations and execution time (PLC)",
       x = "Iterations", y = "Time") +
  geom_smooth(method = "lm")

summary(lm(d.corr.plc.no.to$TimeMs ~ d.corr.plc.no.to$Iterations))
```

# Pairwise comparisons

In the following, I analyze the effect of individual parameters of the algorithm. This is done by forming pairs from the measurements (using the join operation known from databases), where each parameter is the same, except the examined one. Then, a scatterplot is drawn, where the `x` and `y` values represent the two possible parameter values. Note, that this only works for parameters with 2 factors, but in my case, most of the parameters fulfill this constraint (e.g., the domain can be predicate or explicit).

## Comparison of execution time for predicate and explicit domains

Each point in the following plot represents two executions for the same model: one with predicate domain and one with explicit domain. A point above the line means that predicate domain was faster, while a point below the line means the opposite. Points at the right and top edges of the plot correspond to timeouts. It can be seen that verification of PLC models is more efficient in the explicit domain. Hardware models show some diversity, each domain has good results. Some clusters can also be observed, therefore I also performed cluster analysis.

```{r}
# Select only input variables and time
d.inputs.time <- select(d, Type, Model, Domain, Refinement, InitPrec, Search, TimeMs)
# Fill NAs with timeout value
d.inputs.time[is.na(d.inputs.time)] <- timeout.ms
# Filter for the different domains and join by the other columns
d.domain.time <- inner_join(
  filter(d.inputs.time, Domain == "PRED"),
  filter(d.inputs.time, Domain == "EXPL"),
  by = c("Type", "Model", "Refinement", "InitPrec", "Search"))
# Filter where both times are timeout
d.domain.time <- filter(d.domain.time,
                        TimeMs.x !=  timeout.ms | TimeMs.y !=  timeout.ms)

# Plot
ggplot(d.domain.time, aes(TimeMs.x, TimeMs.y, color = Type)) +
  scale_y_log10() + scale_x_log10() +
  geom_bin2d(bins = 12) +
  scale_fill_gradient(low = "gray", high = "black") + 
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")
```

### Clustering

Usually, the first task in cluster analysis is to determine the number of clusters. The following plot shows that the increase in the quality of clustering slows after 3 clusters. Therefore, I ran a k-means cluster analysis with `k = 3`.

```{r}
# Extract data for clustering
d.dom.time.clust <- log10(select(d.domain.time, TimeMs.x, TimeMs.y))
# Determine number of clusters
set.seed(1)
wss <- (nrow(d.dom.time.clust) - 1) * sum(apply(d.dom.time.clust, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(d.dom.time.clust, centers = i)$withinss)
ggplot(data.frame(x = 1:15, wss), aes(x, wss)) +
  geom_point(size = 3) + geom_line() +
  labs(x = "Number of Clusters", y = "Within groups sum of squares")
```

The following plot shows a convincing result.

```{r}
# K-Means Cluster Analysis
fit <- kmeans(d.dom.time.clust, 3)
# Append clusters
d.dom.time.clust <- data.frame(d.domain.time, cluster = as.factor(fit$cluster))
# Plot
ggplot(d.dom.time.clust, aes(TimeMs.x, TimeMs.y, color = cluster)) +
  scale_y_log10() + scale_x_log10() +
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")
```

The table shows that all PLC models are in the same cluster (mixed with some hardware) and there are two clusters containing the majority of hardware models.

```{r}
table(d.dom.time.clust$cluster, d.dom.time.clust$Type)
```

As it can be seen below, running clustering with `k = 5` only yields a slightly better classification (there are a bit less hardware mixed with PLC).

```{r}
d.dom.time.clust <- log10(select(d.domain.time, TimeMs.x, TimeMs.y))
# K-Means Cluster Analysis
fit <- kmeans(d.dom.time.clust, 5)
# Append clusters
d.dom.time.clust <- data.frame(d.domain.time, cluster = as.factor(fit$cluster))
# Plot
ggplot(d.dom.time.clust, aes(TimeMs.x, TimeMs.y, color = cluster)) +
  scale_y_log10() + scale_x_log10() +
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")

table(d.dom.time.clust$cluster, d.dom.time.clust$Type)
```

## Comparison of iterations for Craig and sequence itp refinements

Each point in the following plot represents two executions for the same model: one with Craig interpolation-based refinement and one with sequence interpolation. It can be observed, that for hardware models, sequence interpolation yields slightly less iterations. However, for PLC models, there are much less iterations with sequence interpolation.

```{r}
# Select only input variables and iterations
d.inputs.iters <- select(d, Type, Model, Domain, Refinement,
                         InitPrec, Search, Iterations)
d.inputs.iters <- filter(d.inputs.iters, !is.na(Iterations))
# Filter for the different refinements and join by the other columns
d.refin.iters <- inner_join(
  filter(d.inputs.iters, Refinement == "CRAIG_ITP"),
  filter(d.inputs.iters, Refinement == "SEQ_ITP"),
  by = c("Type", "Model", "Domain", "InitPrec", "Search"))

# Plot
ggplot(d.refin.iters, aes(Iterations.x, Iterations.y, color = Type)) +
  geom_bin2d(bins = 12) +
  scale_fill_gradient(low = "gray", high = "black") + 
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Iterations for different refinements",
       x = "Craig itp", y = "Seqence itp")
```

The previous observation can also be confirmed with a Wilcoxon test: checking whether the number of iterations has identical distribution for different refinement strategies. The p-value is lower than 0.05, therefore the null hypothesis can be rejected, the distribution of the number of iterations is different for the refinement strategies.

```{r}
wilcox.test(Iterations ~ Refinement ,data = filter(d.no.to, Refinement != "SEQ_ITP"))
```

# Decision tree

I also created some decision trees to check which input parameters influence certain output parameters.

## Success of verification

Most of the leaves of the following tree categorize successful (SUCC) and timeout (TO) executions quite well. For example, choosing predicate abstraction for PLCs will most likely not succeed. On the other hand, it is likely to succeed for hardware models. It can also be seen that explicit abstraction with Craig interpolation refinement is likely to succeed regardless of the model type.

```{r fig.width = 12, fig.height = 6}
# Create success column
d.succ <- data.frame(d, Success = ifelse(!is.na(d$TimeMs), "SUCC", "TO") )
# Generate tree
tree <- ctree(Success ~ Type + Domain + Refinement + InitPrec + Search, data = d.succ)
plot(tree, gp = gpar(fontsize = 10), tp_args = list(fill = c("red", "green")))
```

## Counterexample length

I also checked if the search strategy (BFS or DFS) has an effect on the length of the counterexample. However, as the following tree shows, for hardware models, counterexamples are short regardless of the search strategy and there is no significant difference for PLCs as well: with the other parameters adjusted, DFS can also produce shorter counterexamples.

```{r fig.width = 12, fig.height = 6}
# Filter for unsafe executions
d.cex <- d %>% filter(!is.na(CEXlen))
# Generate tree
tree <- ctree(CEXlen ~ Type + Domain + Refinement + InitPrec + Search, data = d.cex)
plot(tree, type = "simple", gp = gpar(fontsize = 8))
```

The previous observation can be checked by a Wilcoxon test: checking whether the counterexample length has identical distribution for different search strategies.

```{r}
wilcox.test(CEXlen ~ Search, data = d.cex) 
```

The p-value is large, which means that there is no strong evidence stating that DFS and BFS would yield counterexamples with different length.

# Conclusion

In my homework I analyzed different configurations of a formal verification algorithm on various models. I checked basic properties of the configurations, examined correlations, performed pairwise comparisons and clustering, and I also generated decision trees. The results showed some interesting properties of the algorithm configurations. In the future I am planning to optimize my algorithms so that I can successfully run them on a larger number of models, yielding more robust results.