---
title: "IDA Homework"
author: "Akos Hajdu"
date: '2016'
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
  html_document: default
classoption: a4paper
---
# Introduction

## Background

In my research, I am working on the formal verification of software and hardware systems. I create a formal model (a formal representation) of the system and the desired property using first order logic formulas. Then, the set of possible states of the system (state space) can be explored to check if the desired property holds for each reachable state. However, a major drawback of formal verification techniques is the so-called state space explosion problem, which means that the set of possible states of a system can be unmanageably or even infinitely large. To overcome this problem, I use abstraction-based techniques, which reduce complexity by hiding information that is not relevant for verification. However, finding the proper precision of abstraction is a difficult task. Counterexample-Guided Abstraction Refinement (CEGAR) is an automatic verification algorithm that starts with a coarse abstraction and refines it iteratively until the proper precision is obtained. CEGAR is a general concept, having numerous variants in the literature. However, different variants perform better for different tasks.

In my research I am developing a generic CEGAR framework that can incorporate different variants of the CEGAR algorithm. In this homework I run several variants of the algorithm on different models and analyze the results.

## Variables of the problem

### Input variables

* Parameters of the model (the formal representation of the system under verification)
    + **Type** of the model (hardware or software)
    + Name of the **model**
    + Number of **variables** in the model
    + **Size** of the formulas describing the model
* Parameters of the algorithm
    + Abstract **domain** (predicate or explicit)
    + **Refinement** strategy (Craig itp., sequence itp., unsat core)
    + **Initial precision** of the abstraction (empty or property-based)
    + **Search** strategy (BFS or DFS)

*Constraints: unsat core refinement cannot be used in predicate domain. Besides that, all combinations of the algorithm parameters are valid.*

### Output variables

* Is the model **safe**, i.e., does the property hold
* Execution **time**
* Number of refinement **iterations**
* **Size** of the ARG (Abstract Reachability Graph, i.e., the abstract state space)
* **Depth** of the ARG
* **Length** of the counterexample (if the model is not safe)

*Constraints: it is possible that the algorithm did not terminate within a specified time (see later). In this case all output variables are `NA` values. Furthermore, the lenght of the counterexample is `NA` if the model is safe.*

# Loading data, initialization

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(partykit)
library(cluster)
library(corrgram)
library(lubridate)
library(gridExtra)
```

## Common variables for the script

```{r}
timeout.ms <- 480000                           # Timeout used during the measurements
csv.path = "log_20161209_153530_hw_plc_vm.csv" # Path of the data

```

## Load and clean data, summarize
 
```{r}
d <- read.csv(csv.path, header = T, sep = ",", quote = "", na.strings = "")
# Formatting: trim the name of the model, determine new variable 'Type'
d$Model <- as.factor(gsub("models/", "", d$Model))
d$Model <- as.factor(substr(d$Model, 0, regexpr("\\.[^\\.]*$", d$Model) - 1))
d <- data.frame(Type = as.factor(substr(d$Model, 0, regexpr('/', d$Model) - 1)), d)
# Filter timeouts
d.no.to <- d %>% filter(!is.na(TimeMs))
```

```{r, include = FALSE}
models.count <- length(unique(d$Model))
configs.count <- nrow(unique(select(d, Domain, Refinement, InitPrec, Search)))
total.time <- seconds_to_period(sum(ifelse(is.na(d$TimeMs), timeout.ms, d$TimeMs))/1000)
```

There are **`r models.count`** input models and **`r configs.count`** algorithm configurations. Each measurement was repeated **`r nrow(d)/models.count/configs.count`** times, yielding a total number of **`r nrow(d)`** measurement points with **`r nrow(d.no.to)`** non-timeouts (**`r nrow(d.no.to)/nrow(d)*100`%**). Total time of the measurements: **`r total.time$day` days `r total.time$hour` hours**.

Details:

```{r}
str(d, width = 80, strict.width = "cut") # Check column types
summary(d) # Check summary values
```

# Basic analysis

## Histograms of output variables

```{r}
plots = list()
plots[[1]] <- qplot(d.no.to$Safe,
                    xlab = "Safety")
plots[[2]] <- qplot(d.no.to$TimeMs, geom = "histogram", bins = 10,
                    xlab = "Execution time (ms)")
plots[[3]] <- qplot(d.no.to$Iterations, geom = "histogram", bins = 10,
                    xlab = "Iterations")
plots[[4]] <- qplot(d.no.to$ARGsize, geom = "histogram", bins = 10,
                    xlab = "ARG size")
plots[[5]] <- qplot(d.no.to$ARGdepth, geom = "histogram", bins = 10,
                    xlab = "ARG depth")
plots[[6]] <- qplot(filter(d.no.to, !is.na(d.no.to$CEXlen))$CEXlen,
                    geom = "histogram", bins = 20, xlab = "Counterexample length")
do.call(grid.arrange, plots)
```

## Number of successful executions
```{r}
configs <- paste(d$Domain, d$Refinement, d$InitPrec, d$Search, sep = " ")
model.conf.succ <- data.frame(Config = configs, Model = d$Model,
                              Success = ifelse(is.na(d$Safe), 0, 1))
model.conf.succ.summ <- model.conf.succ %>% group_by(Config, Model) %>%
  summarise(Success = sum(Success))
ggplot(model.conf.succ.summ, aes(Model, Config, fill = Success)) +
  geom_tile(colour = "darkgray") + scale_fill_gradient(low = "red", high = "green") +
  labs(title = "Number of successful (non timeout) executions",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot above shows that each model was verified by at least one configuration. It is interesting that there was a model, which could only be verified by a single configuration. It can also be observed, that either all executions of configuration succeeded or all timed out. Regarding the configurations, it can be seen that some of them can verify almost every model, while others can only verify about half ot them. However, none of the configurations could verify every model.

## Safety results
```{r}
config.safe <- data.frame(Config = configs, Model = d$Model,
                          Safe = ifelse(d$Safe == "true",1,0))
config.safe.summ <- config.safe %>% group_by(Config, Model) %>%
  summarise(Safe = sum(Safe))

ggplot(config.safe.summ, aes(Model, Config, fill = Safe)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "red", high = "green", na.value = "white") +
  labs(title = "Number of executions that yielded a safe result",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot above shows that (1) all configurations agree on the safeness of the model (whether it meets the property or not) and (2) either all executions of a configuration yielded safe or all yielded unsafe. The lack of the previous properties would obviously mean that the algorithms are not sound.

## Average execution times
```{r}
config.time <- data.frame(Config = configs, Model = d$Model, TimeMs = d$TimeMs)
config.time.summ <- config.time %>% group_by(Config, Model) %>%
  summarise(AvgTime = log10(mean(TimeMs)), RSD = sd(TimeMs)/mean(TimeMs))

ggplot(config.time.summ, aes(Model, Config, fill = AvgTime)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  labs(title = "Average execution time (log10, milliseconds)",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot above shows that there are 2-3 orders of magnitude difference between execution times. It is not surprising that the same configuration performs differently for different models, but it is interesting that for certain models there is a great difference in performance with the change of a single parameter. It is also quite surprising that there is a model, where only one configuration was successful, but with a short execution time. This case could be the subject of detailed analysis in the future.


## Relative standard deviation of measurements
```{r}
ggplot(config.time.summ, aes(Model, Config, fill = RSD)) +
  geom_tile(colour = "darkgray") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  labs(title = "Relative standard deviation (RSD) of time",
       x = "Model", y = "Configuration") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot above shows that the relative standard deviation of the repeated executions is low (maximum is `r max(config.time.summ$RSD, na.rm = TRUE)`). This indicates a robust measurement environment.

# Correlations

## On the whole data set
```{r}
# Select numerical data
d.corr <- select(d, TimeMs, Vars, Size, Iterations, ARGsize, ARGdepth, CEXlen)
cor(d.corr, use = "pairwise.complete.obs")
corrgram(d.corr, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
corrgram(d.corr, order = TRUE, lower.panel = panel.ellipse, upper.panel = panel.pts)
```

The matrices above show some correlation between variables, but it is not convincing enough. Therefore, I split the results into two parts based on the type of the model (hardware or PLC).

## On hardware models
```{r}
d.corr.hw <- select(filter(d, Type == "hw"), TimeMs, Vars, Size, Iterations, ARGsize,
                    ARGdepth, CEXlen)
cor(d.corr.hw, use = "pairwise.complete.obs")
corrgram(d.corr.hw, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
```

The diagrams above show some strong correlations.
* Correlation between the number of variables and the size of the model is due to the formal representation of hardware circuits (e.g., for each gate output, a unique variable is assigned).
* The correlation between the depth of the ARG and the lenght of the counterexample may be due to the fact that after finding the counterexample, the search is stopped.
* Correlation between the ARG size and execution time is not surprising.
* Correlation between the number of iterations and the counterexample length indicates that for each step towards the target state, a refinement was required.

## On PLC models
```{r}
d.corr.plc <- select(filter(d, Type == "plc"), TimeMs, Vars, Size, Iterations, ARGsize,
                     ARGdepth, CEXlen)
cor(d.corr.plc, use = "pairwise.complete.obs")
corrgram(d.corr.plc, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
```

Correlations for PLC models are less significant. This may be due to the high diversity of PLC models.

## Linear regressions

Based on the correlations above, some linear regressions were also calculated. The R-squared values show a strong correlation (R-squared > 0.8).

### Hardware models
```{r}
ggplot(d.corr.hw, aes(Vars, Size)) + geom_point() +
  labs(title = "Linear regression between variables and size (hw)",
       x = "Variables", y = "Size") +
  geom_smooth(method = "lm")

summary(lm(d.corr.hw$Size ~ d.corr.hw$Vars))

d.corr.hw.no.to <- filter(d.corr.hw, !is.na(TimeMs))
ggplot(d.corr.hw.no.to, aes(ARGsize, TimeMs)) +
  geom_point() +
  labs(title = "Linear regression between ARG size and time (hw)",
       x = "ARG size", y = "Time") +
  geom_smooth(method = "lm")

summary(lm(d.corr.hw.no.to$TimeMs ~ d.corr.hw.no.to$ARGsize))
```
### PLC models

```{r}
d.corr.plc.no.to <- filter(d.corr.plc, !is.na(TimeMs))
ggplot(d.corr.plc.no.to, aes(Iterations, TimeMs)) +
  geom_point() +
  labs(title = "Linear regression between iterations and execution time (PLC)",
       x = "Iterations", y = "Time") +
  geom_smooth(method = "lm")

summary(lm(d.corr.plc.no.to$TimeMs ~ d.corr.plc.no.to$Iterations))
```

# Pairwise comparisons

In the following, the effect of individual parameters of the algorithm are examined. This is done by forming pairs from the measurements (using the join operation known from databases), where each parameter is the same, except the examined one. Then, a scatterplot is drawn, where the `x` and `y` values represent the first and second parameter value. Note, that this only works for parameters with 2 factors, but in my case, most of the input parameters fulfill this constraint (e.g., the domain can be predicate or explicit).

## Comparison of exectuion time for predicate and explicit domains

```{r}
# Select only input variables and time
d.inputs.time <- select(d, Type, Model, Domain, Refinement, InitPrec, Search, TimeMs)
# Fill NAs with timeout value
d.inputs.time[is.na(d.inputs.time)] <- timeout.ms
# Filter for the different domains and join by the other columns
d.domain.time <- inner_join(
  filter(d.inputs.time, Domain == "PRED"),
  filter(d.inputs.time, Domain == "EXPL"),
  by = c("Type", "Model", "Refinement", "InitPrec", "Search"))
# Filter where both times are timeout
d.domain.time <- filter(d.domain.time,
                        TimeMs.x !=  timeout.ms | TimeMs.y !=  timeout.ms)

# Plot
ggplot(d.domain.time, aes(TimeMs.x, TimeMs.y, color = Type)) +
  scale_y_log10() + scale_x_log10() +
  geom_bin2d(bins = 12) +
  scale_fill_gradient(low = "gray", high = "black") + 
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")
```

Each point in the plot above represents two executions for the same model: one with predicate domain and one with explicit domain. A point above the line means that predicate domain was faster, while a point below the line means the opposite. Points at the right and top edges of the plot correspont to timeouts. It can be seen that verification of PLC models is more efficient in the explicit domain. Hardware models show some diversity, each domain has good results. Some clusters can also be observed, therefore cluster analysis was also performed.

### Clustering

Usually, the first task in cluster analysis is to determine the number of clusters.

```{r}
# Extract data for clustering
d.dom.time.clust <- log10(select(d.domain.time, TimeMs.x, TimeMs.y))
# Determine number of clusters
set.seed(1)
wss <- (nrow(d.dom.time.clust)-1)*sum(apply(d.dom.time.clust,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(d.dom.time.clust,centers = i)$withinss)
ggplot(data.frame(x = 1:15, wss), aes(x, wss)) +
  geom_point(size = 3) + geom_line() +
  labs(x = "Number of Clusters", y = "Within groups sum of squares")
```

The plot above shows that the increase in the quality of clustering slows after 3 clusters. Therefore, cluster analysis was run with `k = 3`.

```{r}
# K-Means Cluster Analysis
fit <- kmeans(d.dom.time.clust, 3)
# Append clusters
d.dom.time.clust <- data.frame(d.domain.time, cluster = as.factor(fit$cluster))
# Plot
ggplot(d.dom.time.clust, aes(TimeMs.x, TimeMs.y, color = cluster)) +
  scale_y_log10() + scale_x_log10() +
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")
# Table
table(d.dom.time.clust$cluster, d.dom.time.clust$Type)
```

The plot shows a convincing result. The table shows that all PLC models are in the same group (mixed with some hardware), but the majority of hardware models belong to two different clusters.

As it can be seen below, running clustering with `k = 5` only yields a slightly better classification (there are a bit less hardware mixed with PLC).

```{r}
d.dom.time.clust <- log10(select(d.domain.time, TimeMs.x, TimeMs.y))
# K-Means Cluster Analysis
fit <- kmeans(d.dom.time.clust, 5)
# Append clusters
d.dom.time.clust <- data.frame(d.domain.time, cluster = as.factor(fit$cluster))
# Plot
ggplot(d.dom.time.clust, aes(TimeMs.x, TimeMs.y, color = cluster)) +
  scale_y_log10() + scale_x_log10() +
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Execution time of different domains", x = "Pred", y = "Expl")
# Table
table(d.dom.time.clust$cluster, d.dom.time.clust$Type)
```

## Comparison of iterations for Craig and sequence itp refinements
```{r}
# Select only input variables and iterations
d.inputs.iters <- select(d, Type, Model, Domain, Refinement,
                         InitPrec, Search, Iterations)
d.inputs.iters <- filter(d.inputs.iters, !is.na(Iterations))
# Filter for the different refinements and join by the other columns
d.refin.iters <- inner_join(
  filter(d.inputs.iters, Refinement == "CRAIG_ITP"),
  filter(d.inputs.iters, Refinement == "SEQ_ITP"),
  by = c("Type", "Model", "Domain", "InitPrec", "Search"))

# Plot
ggplot(d.refin.iters, aes(Iterations.x, Iterations.y, color = Type)) +
  geom_bin2d(bins = 12) +
  scale_fill_gradient(low = "gray", high = "black") + 
  geom_point(alpha = 1/5, size = 3) +
  geom_abline() +
  labs(title = "Iterations for different refinements",
       x = "Craig itp", y = "Seqence itp")
```

Each point in the plot above represents two executions for the same model: one with Craig interpolation-based refinement and one with sequence interpolation. It can be observed, that for hardware models, sequence interpolation yields slightly less iterations. However, for PLC models, there are much less iterations with sequence interpolation.

# Decision tree

I also created some decision trees to check which input parameters influence certain output parameters.

## Success of verification
```{r fig.width = 12}
d.succ <- data.frame(d, Success = ifelse(!is.na(d$TimeMs), "SUCC", "TO") )
tree <- ctree(Success ~ Type + Domain + Refinement + InitPrec + Search, data = d.succ)
plot(tree, gp = gpar(fontsize = 10))
```

Most of the leaves of the tree above categorize successful (SUCC) and timeout (TO) executions quite well. For example, choosing predicate abstraction for PLCs will most likely not succeed. On the other hand, it is likely to succeed for hardware models. It can also be seen that explicit abstraction with Craig or sequence itp. refinement is likely to succeed regardless of the model type.


## Counterexample length
```{r fig.width = 12}
d.cex <- d %>% filter(!is.na(CEXlen))
tree <- ctree(CEXlen ~ Type + Domain + Refinement + InitPrec + Search, data = d.cex)
plot(tree, type = "simple", gp = gpar(fontsize = 8))
```

I also checked if the search strategy (BFS or DFS) has an effect on the lenght of the counterexample. However, as the tree shows above, for hardware models, counterexamples are short regardless of the search strategy and there is no significant difference for PLCs as well: with the other parameters adjusted, DFS can also produce shorter counterexamples. This can be verified by a Wilcoxon test: checking whether the counterexample length has identical distribution for different search strategies.

```{r}
wilcox.test(CEXlen ~ Search, data = d.cex) 
```

The p-value is large, which means that I accept the null hypothesis.
